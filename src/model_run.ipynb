{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "from src import preproc, feed, model_fn\n",
    "from tqdm import tqdm_notebook\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "219162it [00:01, 127410.33it/s]\n"
     ]
    }
   ],
   "source": [
    "data = preproc.read_data(db_path='data/jokes.db', duplicates=False)\n",
    "data_len = len(data)\n",
    "data = data[:data_len//3]\n",
    "vocab = preproc.Vocab(data)\n",
    "char2idx, idx2char = vocab.build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_DIR = 'config/'\n",
    "LOG_DIR = 'logs/'\n",
    "\n",
    "params = {\n",
    "    'vocab': char2idx,\n",
    "    \n",
    "    \"encoder_lstm_size\": 200,\n",
    "    \"encoder_num_layers\": 2,\n",
    "    \"encoder_final_context_dims\": 200,\n",
    "    \"encoder_vocab_size\": len(char2idx),\n",
    "    \"encoder_state_size\": 60,\n",
    "    \"encoder_emb_size\": 100,\n",
    "    \"encoder_vocab\": char2idx,\n",
    "    \"initial_state_zero\": True,\n",
    "\n",
    "    \"decoder_lstm_size\": 200,\n",
    "    \"decoder_vocab_size\": len(char2idx),\n",
    "    \"decoder_max_len\": 300,\n",
    "    \"decoder_vocab\": char2idx,\n",
    "    \"reverse_decoder_vocab\": idx2char,\n",
    "    \"decoder_emb_size\": 100,\n",
    "\n",
    "    \"use_encoder_embeddings\": True,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 128,\n",
    "    \"clip\": .5\n",
    "}\n",
    "\n",
    "with open(os.path.join(CONFIG_DIR, 'params.json'), 'w') as f:\n",
    "    json.dump(params, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b54120dc3141afbab6868f5447bd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=73054), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train size: 65748 0.8999917868973636\n",
      "val size: 3653 0.050004106551318206\n",
      "dev size: 3653 0.050004106551318206\n"
     ]
    }
   ],
   "source": [
    "docs_ids = [vocab.tokenize(text) for text in tqdm_notebook(data)]\n",
    "\n",
    "docs_train, docs_val = train_test_split(\n",
    "    docs_ids, test_size=0.10, random_state=42)\n",
    "docs_val, docs_dev = train_test_split(\n",
    "    docs_val, test_size=0.5, random_state=42)\n",
    "\n",
    "print('train size:', len(docs_train), len(docs_train)/len(docs_ids))\n",
    "print('val size:', len(docs_val), len(docs_val)/len(docs_ids))\n",
    "print('dev size:', len(docs_dev), len(docs_dev)/len(docs_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train = shuffle(docs_train, random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_ = feed.Feed()\n",
    "train_input_fn = feed_.input_fn(docs_train, docs_train, params)\n",
    "eval_input_fn = feed_.input_fn(docs_val, docs_val, params)\n",
    "predict_input_fn = feed_.predict_input_fn(params=params, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'logs/model1/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 3600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 150, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77554b3080>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "cfg = tf.estimator.RunConfig(\n",
    "    keep_checkpoint_max=150,\n",
    "    save_checkpoints_secs=60*60,\n",
    ")\n",
    "estimator = tf.estimator.Estimator(model_fn.model_fn, os.path.join(LOG_DIR, 'model1/'), cfg, params)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, throttle_secs=120*60)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize cell states as zeros\n",
      "INFO:tensorflow:<tf.Variable 'decoder_embeddings:0' shape=(106, 100) dtype=float32_ref>\n",
      "INFO:tensorflow:Tensor(\"Fill:0\", shape=(?,), dtype=int32)\n",
      "INFO:tensorflow:Tensor(\"ToInt32_1/x:0\", shape=(), dtype=int32)\n",
      "INFO:tensorflow:Tensor(\"dense/BiasAdd:0\", shape=(?, ?, 106), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The two structures don't have the same nested structure.\n\nFirst structure: type=Tensor str=Tensor(\"dense/BiasAdd:0\", shape=(?, ?, 106), dtype=float32)\n\nSecond structure: type=tuple str=(200, 200)\n\nMore specifically: Substructure \"type=tuple str=(200, 200)\" is a sequence, while substructure \"type=Tensor str=Tensor(\"dense/BiasAdd:0\", shape=(?, ?, 106), dtype=float32)\" is not\nEntire first structure:\n.\nEntire second structure:\n(., .)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36massert_same_structure\u001b[0;34m(nest1, nest2, check_types)\u001b[0m\n\u001b[1;32m    178\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAssertSameStructure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnest2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The two structures don't have the same nested structure.\n\nFirst structure: type=Tensor str=Tensor(\"dense/BiasAdd:0\", shape=(?, ?, 106), dtype=float32)\n\nSecond structure: type=tuple str=(200, 200)\n\nMore specifically: Substructure \"type=tuple str=(200, 200)\" is a sequence, while substructure \"type=Tensor str=Tensor(\"dense/BiasAdd:0\", shape=(?, ?, 106), dtype=float32)\" is not",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-97960854c9fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m    609\u001b[0m             input_fn, model_fn_lib.ModeKeys.PREDICT)\n\u001b[1;32m    610\u001b[0m         estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 611\u001b[0;31m             features, None, model_fn_lib.ModeKeys.PREDICT, self.config)\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;31m# Call to warm_start has to be after model_fn is called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/studyspace/tinkoff_fintech/week7/jokes_hw/src/model_fn.py\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprediction_initial_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mbeam_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0moutput_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_projection_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         )\n\u001b[1;32m    194\u001b[0m         prediction_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cell, embedding, start_tokens, end_token, initial_state, beam_width, output_layer, length_penalty_weight, coverage_penalty_weight, reorder_tensor_arrays)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coverage_penalty_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoverage_penalty_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     self._initial_cell_state = nest.map_structure(\n\u001b[0;32m--> 341\u001b[0;31m         self._maybe_split_batch_beams, initial_state, self._cell.state_size)\n\u001b[0m\u001b[1;32m    342\u001b[0m     self._start_tokens = array_ops.tile(\n\u001b[1;32m    343\u001b[0m         array_ops.expand_dims(self._start_tokens, 1), [1, self._beam_width])\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **check_types_dict)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mother\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m     \u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m   \u001b[0mflat_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36massert_same_structure\u001b[0;34m(nest1, nest2, check_types)\u001b[0m\n\u001b[1;32m    184\u001b[0m                   \u001b[0;34m\"Entire first structure:\\n%s\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                   \u001b[0;34m\"Entire second structure:\\n%s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                   % (str(e), str1, str2))\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The two structures don't have the same nested structure.\n\nFirst structure: type=Tensor str=Tensor(\"dense/BiasAdd:0\", shape=(?, ?, 106), dtype=float32)\n\nSecond structure: type=tuple str=(200, 200)\n\nMore specifically: Substructure \"type=tuple str=(200, 200)\" is a sequence, while substructure \"type=Tensor str=Tensor(\"dense/BiasAdd:0\", shape=(?, ?, 106), dtype=float32)\" is not\nEntire first structure:\n.\nEntire second structure:\n(., .)"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for p in estimator.predict(input_fn=predict_input_fn):\n",
    "    predictions.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
